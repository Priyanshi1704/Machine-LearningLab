{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ml assignment - 9 SLP for multiclass classification .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Priyanshi1704/Machine-LearningLab/blob/main/Ml_assignment_9_SLP_for_multiclass_classification_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtslkPKz6hTU"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "import warnings\n",
        "from matplotlib import pyplot as plt\n",
        "warnings.filterwarnings('default')\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjBfQHGjj7d3"
      },
      "source": [
        "# DataSet Loading and Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2NiDqXdmkqD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c761938f-e600-418e-e98f-045c26bba43f"
      },
      "source": [
        "# Iris Dataset\n",
        "from sklearn import datasets\n",
        "data = datasets.load_iris()\n",
        "'''\n",
        "# Wine Dataset\n",
        "from sklearn.datasets import load_wine\n",
        "data = load_wine()\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# Wine Dataset\\nfrom sklearn.datasets import load_wine\\ndata = load_wine()\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAgtd79DmktA",
        "outputId": "859acb5d-1dd2-4de7-a8f7-b36228356c4d"
      },
      "source": [
        "print(\"The dictonary keys are associated with data:\\n\", data.keys())\n",
        "print(\"-\"*100)\n",
        "print(\"Data:\\n\", data['data'])\n",
        "print(\"-\"*100)\n",
        "print(\"Name of the features (Independent features): \\n\", data['feature_names'])\n",
        "print(\"-\"*100)\n",
        "print(\"Name of the dependent features: \", data['target_names'])\n",
        "print(\"-\"*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dictonary keys are associated with data:\n",
            " dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Data:\n",
            " [[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]\n",
            " [5.4 3.9 1.7 0.4]\n",
            " [4.6 3.4 1.4 0.3]\n",
            " [5.  3.4 1.5 0.2]\n",
            " [4.4 2.9 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.4 3.7 1.5 0.2]\n",
            " [4.8 3.4 1.6 0.2]\n",
            " [4.8 3.  1.4 0.1]\n",
            " [4.3 3.  1.1 0.1]\n",
            " [5.8 4.  1.2 0.2]\n",
            " [5.7 4.4 1.5 0.4]\n",
            " [5.4 3.9 1.3 0.4]\n",
            " [5.1 3.5 1.4 0.3]\n",
            " [5.7 3.8 1.7 0.3]\n",
            " [5.1 3.8 1.5 0.3]\n",
            " [5.4 3.4 1.7 0.2]\n",
            " [5.1 3.7 1.5 0.4]\n",
            " [4.6 3.6 1.  0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.8 3.4 1.9 0.2]\n",
            " [5.  3.  1.6 0.2]\n",
            " [5.  3.4 1.6 0.4]\n",
            " [5.2 3.5 1.5 0.2]\n",
            " [5.2 3.4 1.4 0.2]\n",
            " [4.7 3.2 1.6 0.2]\n",
            " [4.8 3.1 1.6 0.2]\n",
            " [5.4 3.4 1.5 0.4]\n",
            " [5.2 4.1 1.5 0.1]\n",
            " [5.5 4.2 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.2]\n",
            " [5.  3.2 1.2 0.2]\n",
            " [5.5 3.5 1.3 0.2]\n",
            " [4.9 3.6 1.4 0.1]\n",
            " [4.4 3.  1.3 0.2]\n",
            " [5.1 3.4 1.5 0.2]\n",
            " [5.  3.5 1.3 0.3]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [4.4 3.2 1.3 0.2]\n",
            " [5.  3.5 1.6 0.6]\n",
            " [5.1 3.8 1.9 0.4]\n",
            " [4.8 3.  1.4 0.3]\n",
            " [5.1 3.8 1.6 0.2]\n",
            " [4.6 3.2 1.4 0.2]\n",
            " [5.3 3.7 1.5 0.2]\n",
            " [5.  3.3 1.4 0.2]\n",
            " [7.  3.2 4.7 1.4]\n",
            " [6.4 3.2 4.5 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [5.5 2.3 4.  1.3]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [5.7 2.8 4.5 1.3]\n",
            " [6.3 3.3 4.7 1.6]\n",
            " [4.9 2.4 3.3 1. ]\n",
            " [6.6 2.9 4.6 1.3]\n",
            " [5.2 2.7 3.9 1.4]\n",
            " [5.  2.  3.5 1. ]\n",
            " [5.9 3.  4.2 1.5]\n",
            " [6.  2.2 4.  1. ]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [5.6 2.9 3.6 1.3]\n",
            " [6.7 3.1 4.4 1.4]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.8 2.7 4.1 1. ]\n",
            " [6.2 2.2 4.5 1.5]\n",
            " [5.6 2.5 3.9 1.1]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [6.1 2.8 4.  1.3]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.8 4.7 1.2]\n",
            " [6.4 2.9 4.3 1.3]\n",
            " [6.6 3.  4.4 1.4]\n",
            " [6.8 2.8 4.8 1.4]\n",
            " [6.7 3.  5.  1.7]\n",
            " [6.  2.9 4.5 1.5]\n",
            " [5.7 2.6 3.5 1. ]\n",
            " [5.5 2.4 3.8 1.1]\n",
            " [5.5 2.4 3.7 1. ]\n",
            " [5.8 2.7 3.9 1.2]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.  3.4 4.5 1.6]\n",
            " [6.7 3.1 4.7 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [5.6 3.  4.1 1.3]\n",
            " [5.5 2.5 4.  1.3]\n",
            " [5.5 2.6 4.4 1.2]\n",
            " [6.1 3.  4.6 1.4]\n",
            " [5.8 2.6 4.  1.2]\n",
            " [5.  2.3 3.3 1. ]\n",
            " [5.6 2.7 4.2 1.3]\n",
            " [5.7 3.  4.2 1.2]\n",
            " [5.7 2.9 4.2 1.3]\n",
            " [6.2 2.9 4.3 1.3]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [5.7 2.8 4.1 1.3]\n",
            " [6.3 3.3 6.  2.5]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [7.1 3.  5.9 2.1]\n",
            " [6.3 2.9 5.6 1.8]\n",
            " [6.5 3.  5.8 2.2]\n",
            " [7.6 3.  6.6 2.1]\n",
            " [4.9 2.5 4.5 1.7]\n",
            " [7.3 2.9 6.3 1.8]\n",
            " [6.7 2.5 5.8 1.8]\n",
            " [7.2 3.6 6.1 2.5]\n",
            " [6.5 3.2 5.1 2. ]\n",
            " [6.4 2.7 5.3 1.9]\n",
            " [6.8 3.  5.5 2.1]\n",
            " [5.7 2.5 5.  2. ]\n",
            " [5.8 2.8 5.1 2.4]\n",
            " [6.4 3.2 5.3 2.3]\n",
            " [6.5 3.  5.5 1.8]\n",
            " [7.7 3.8 6.7 2.2]\n",
            " [7.7 2.6 6.9 2.3]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.9 3.2 5.7 2.3]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [7.7 2.8 6.7 2. ]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.7 3.3 5.7 2.1]\n",
            " [7.2 3.2 6.  1.8]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.1 3.  4.9 1.8]\n",
            " [6.4 2.8 5.6 2.1]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [7.4 2.8 6.1 1.9]\n",
            " [7.9 3.8 6.4 2. ]\n",
            " [6.4 2.8 5.6 2.2]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.1 2.6 5.6 1.4]\n",
            " [7.7 3.  6.1 2.3]\n",
            " [6.3 3.4 5.6 2.4]\n",
            " [6.4 3.1 5.5 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.9 3.1 5.4 2.1]\n",
            " [6.7 3.1 5.6 2.4]\n",
            " [6.9 3.1 5.1 2.3]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [6.8 3.2 5.9 2.3]\n",
            " [6.7 3.3 5.7 2.5]\n",
            " [6.7 3.  5.2 2.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [6.2 3.4 5.4 2.3]\n",
            " [5.9 3.  5.1 1.8]]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Name of the features (Independent features): \n",
            " ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Name of the dependent features:  ['setosa' 'versicolor' 'virginica']\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBOXSkR2MaGX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1079fe4-0932-4e73-c6a0-19eee926aaca"
      },
      "source": [
        "y_actual = np.array(data['target'])\n",
        "features = np.array(data['data'])\n",
        "print(\"No of the feature vectors: %d\"%features.shape[1])\n",
        "print(\"No of the Patterns: %d\"%features.shape[0])\n",
        "print(\"No of classes:\", list(set(data['target'])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No of the feature vectors: 4\n",
            "No of the Patterns: 150\n",
            "No of classes: [0, 1, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "b3VOsHY6h3HT",
        "outputId": "a8bd78bc-3844-43b1-82ae-8f9da64327e7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "iris = sns.load_dataset(\"iris\")\n",
        "iris[\"ID\"] = iris.index\n",
        "iris[\"ratio\"] = iris[\"sepal_length\"]/iris[\"sepal_width\"]\n",
        "\n",
        "sns.lmplot(x=\"ID\", y=\"ratio\", data=iris, hue=\"species\", fit_reg=False, legend=False)\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAVaYqWDmPyJ"
      },
      "source": [
        "# data preprocessing (min max scalling)\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(features)\n",
        "features = scaler.transform(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dt2MIE22vX9H"
      },
      "source": [
        "# shuffel the datasample\n",
        "from sklearn.utils import shuffle\n",
        "import numpy as np\n",
        "features, y_actual = shuffle(features, y_actual)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bh83KJ9Z3eq"
      },
      "source": [
        "# Train Set, Validation Set and Test Set (Just for checking)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKhh1VCNmd6k",
        "outputId": "7b9851f4-5456-45a5-9189-066e46fb5a71"
      },
      "source": [
        "# Train test validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "# train(70%), validation (10%), and test(20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(features,y_actual , test_size=0.3, random_state=random.randint(30,100))\n",
        "X_test, X_validation, y_test, y_validation = train_test_split(X_test, y_test, test_size=0.14, random_state=random.randint(30,100))\n",
        "print(\"Size of the train dataset:\" + str(X_train.shape))\n",
        "print(\"Size of the validation dataset:\" + str(X_validation.shape))\n",
        "print(\"Size of the test dataset:\" + str(X_test.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the train dataset:(105, 4)\n",
            "Size of the validation dataset:(7, 4)\n",
            "Size of the test dataset:(38, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPVLuQU8ogX2"
      },
      "source": [
        "# 5 fold cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56_KxpXDkkWY"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "def fold(features,y_actual):\n",
        "  kf = KFold(n_splits=5,random_state=1000, shuffle=True)\n",
        "  kf.get_n_splits(features)\n",
        "  print(kf)\n",
        "  all_x_train = []\n",
        "  all_x_test = []\n",
        "  all_y_train = []\n",
        "  all_y_test = []\n",
        "  for train_index, test_index in kf.split(features):\n",
        "    X_train, X_test = features[train_index], features[test_index]\n",
        "    y_train, y_test = y_actual[train_index], y_actual[test_index]\n",
        "    all_x_train.append(X_train)\n",
        "    all_x_test.append(X_test)\n",
        "    all_y_train.append(y_train)\n",
        "    all_y_test.append(y_test)\n",
        "  all_x_train, all_x_test, all_y_train, all_y_test  = np.array(all_x_train), np.array(all_x_test), np.array(all_y_train), np.array(all_y_test)\n",
        "  return all_x_train, all_x_test, all_y_train, all_y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRN8_1KLkoBF",
        "outputId": "37209a3c-f7cb-4deb-9df9-2686f88ab899"
      },
      "source": [
        "all_x_train, all_x_test, all_y_train, all_y_test = fold(features,y_actual)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KFold(n_splits=5, random_state=1000, shuffle=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZAUHI56vDw7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "882f105c-bf8b-44be-b4dc-67a02ae804fa"
      },
      "source": [
        "print(all_x_train, all_x_test, all_y_train, all_y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0.19444444 0.625      0.05084746 0.08333333]\n",
            "  [0.80555556 0.66666667 0.86440678 1.        ]\n",
            "  [0.02777778 0.5        0.05084746 0.04166667]\n",
            "  ...\n",
            "  [0.19444444 0.625      0.10169492 0.20833333]\n",
            "  [0.94444444 0.25       1.         0.91666667]\n",
            "  [0.33333333 0.625      0.05084746 0.04166667]]\n",
            "\n",
            " [[0.19444444 0.625      0.05084746 0.08333333]\n",
            "  [0.80555556 0.66666667 0.86440678 1.        ]\n",
            "  [0.02777778 0.5        0.05084746 0.04166667]\n",
            "  ...\n",
            "  [0.19444444 0.         0.42372881 0.375     ]\n",
            "  [0.55555556 0.29166667 0.66101695 0.70833333]\n",
            "  [0.33333333 0.625      0.05084746 0.04166667]]\n",
            "\n",
            " [[0.19444444 0.625      0.05084746 0.08333333]\n",
            "  [0.80555556 0.66666667 0.86440678 1.        ]\n",
            "  [0.02777778 0.5        0.05084746 0.04166667]\n",
            "  ...\n",
            "  [0.19444444 0.625      0.10169492 0.20833333]\n",
            "  [0.94444444 0.25       1.         0.91666667]\n",
            "  [0.33333333 0.625      0.05084746 0.04166667]]\n",
            "\n",
            " [[0.80555556 0.66666667 0.86440678 1.        ]\n",
            "  [0.25       0.29166667 0.49152542 0.54166667]\n",
            "  [0.55555556 0.58333333 0.77966102 0.95833333]\n",
            "  ...\n",
            "  [0.55555556 0.29166667 0.66101695 0.70833333]\n",
            "  [0.19444444 0.625      0.10169492 0.20833333]\n",
            "  [0.94444444 0.25       1.         0.91666667]]\n",
            "\n",
            " [[0.19444444 0.625      0.05084746 0.08333333]\n",
            "  [0.02777778 0.5        0.05084746 0.04166667]\n",
            "  [0.25       0.29166667 0.49152542 0.54166667]\n",
            "  ...\n",
            "  [0.19444444 0.625      0.10169492 0.20833333]\n",
            "  [0.94444444 0.25       1.         0.91666667]\n",
            "  [0.33333333 0.625      0.05084746 0.04166667]]] [[[0.55555556 0.58333333 0.77966102 0.95833333]\n",
            "  [0.13888889 0.58333333 0.15254237 0.04166667]\n",
            "  [0.44444444 0.41666667 0.69491525 0.70833333]\n",
            "  [0.13888889 0.41666667 0.06779661 0.        ]\n",
            "  [0.13888889 0.45833333 0.10169492 0.04166667]\n",
            "  [0.47222222 0.08333333 0.50847458 0.375     ]\n",
            "  [0.30555556 0.58333333 0.08474576 0.125     ]\n",
            "  [0.52777778 0.08333333 0.59322034 0.58333333]\n",
            "  [0.83333333 0.375      0.89830508 0.70833333]\n",
            "  [0.16666667 0.45833333 0.08474576 0.04166667]\n",
            "  [0.58333333 0.33333333 0.77966102 0.875     ]\n",
            "  [0.38888889 0.75       0.11864407 0.08333333]\n",
            "  [0.61111111 0.41666667 0.81355932 0.875     ]\n",
            "  [0.38888889 0.25       0.42372881 0.375     ]\n",
            "  [0.38888889 0.33333333 0.52542373 0.5       ]\n",
            "  [0.58333333 0.33333333 0.77966102 0.83333333]\n",
            "  [0.33333333 0.25       0.57627119 0.45833333]\n",
            "  [0.61111111 0.41666667 0.71186441 0.79166667]\n",
            "  [0.47222222 0.58333333 0.59322034 0.625     ]\n",
            "  [0.52777778 0.58333333 0.74576271 0.91666667]\n",
            "  [0.66666667 0.54166667 0.79661017 0.83333333]\n",
            "  [0.58333333 0.45833333 0.76271186 0.70833333]\n",
            "  [0.66666667 0.54166667 0.79661017 1.        ]\n",
            "  [0.58333333 0.5        0.72881356 0.91666667]\n",
            "  [0.47222222 0.41666667 0.6440678  0.70833333]\n",
            "  [0.08333333 0.66666667 0.         0.04166667]\n",
            "  [0.91666667 0.41666667 0.94915254 0.83333333]\n",
            "  [0.11111111 0.5        0.05084746 0.04166667]\n",
            "  [0.27777778 0.70833333 0.08474576 0.04166667]\n",
            "  [0.16666667 0.16666667 0.38983051 0.375     ]]\n",
            "\n",
            " [[0.25       0.29166667 0.49152542 0.54166667]\n",
            "  [0.30555556 0.58333333 0.11864407 0.04166667]\n",
            "  [0.61111111 0.33333333 0.61016949 0.58333333]\n",
            "  [0.19444444 0.41666667 0.10169492 0.04166667]\n",
            "  [0.41666667 0.25       0.50847458 0.45833333]\n",
            "  [0.72222222 0.45833333 0.74576271 0.83333333]\n",
            "  [0.47222222 0.29166667 0.69491525 0.625     ]\n",
            "  [0.5        0.33333333 0.50847458 0.5       ]\n",
            "  [0.11111111 0.5        0.10169492 0.04166667]\n",
            "  [0.72222222 0.45833333 0.66101695 0.58333333]\n",
            "  [0.77777778 0.41666667 0.83050847 0.83333333]\n",
            "  [0.69444444 0.33333333 0.6440678  0.54166667]\n",
            "  [0.66666667 0.45833333 0.77966102 0.95833333]\n",
            "  [0.5        0.41666667 0.66101695 0.70833333]\n",
            "  [0.19444444 0.58333333 0.08474576 0.04166667]\n",
            "  [0.55555556 0.375      0.77966102 0.70833333]\n",
            "  [0.30555556 0.79166667 0.11864407 0.125     ]\n",
            "  [0.38888889 0.41666667 0.54237288 0.45833333]\n",
            "  [0.19444444 0.66666667 0.06779661 0.04166667]\n",
            "  [0.55555556 0.54166667 0.84745763 1.        ]\n",
            "  [0.19444444 0.125      0.38983051 0.375     ]\n",
            "  [0.80555556 0.5        0.84745763 0.70833333]\n",
            "  [0.13888889 0.41666667 0.06779661 0.08333333]\n",
            "  [0.02777778 0.375      0.06779661 0.04166667]\n",
            "  [0.22222222 0.70833333 0.08474576 0.125     ]\n",
            "  [0.38888889 0.375      0.54237288 0.5       ]\n",
            "  [0.36111111 0.41666667 0.59322034 0.58333333]\n",
            "  [0.61111111 0.41666667 0.76271186 0.70833333]\n",
            "  [0.19444444 0.625      0.10169492 0.20833333]\n",
            "  [0.94444444 0.25       1.         0.91666667]]\n",
            "\n",
            " [[0.72222222 0.45833333 0.69491525 0.91666667]\n",
            "  [0.33333333 0.20833333 0.50847458 0.5       ]\n",
            "  [0.38888889 0.20833333 0.6779661  0.79166667]\n",
            "  [0.44444444 0.41666667 0.54237288 0.58333333]\n",
            "  [0.66666667 0.41666667 0.6779661  0.66666667]\n",
            "  [0.69444444 0.41666667 0.76271186 0.83333333]\n",
            "  [0.5        0.25       0.77966102 0.54166667]\n",
            "  [0.33333333 0.16666667 0.47457627 0.41666667]\n",
            "  [0.08333333 0.5        0.06779661 0.04166667]\n",
            "  [0.22222222 0.58333333 0.08474576 0.04166667]\n",
            "  [0.69444444 0.5        0.83050847 0.91666667]\n",
            "  [0.47222222 0.08333333 0.6779661  0.58333333]\n",
            "  [0.02777778 0.41666667 0.05084746 0.04166667]\n",
            "  [0.66666667 0.20833333 0.81355932 0.70833333]\n",
            "  [0.25       0.58333333 0.06779661 0.04166667]\n",
            "  [0.22222222 0.20833333 0.33898305 0.41666667]\n",
            "  [0.36111111 0.41666667 0.52542373 0.5       ]\n",
            "  [0.33333333 0.16666667 0.45762712 0.375     ]\n",
            "  [0.66666667 0.45833333 0.62711864 0.58333333]\n",
            "  [0.25       0.875      0.08474576 0.        ]\n",
            "  [0.16666667 0.41666667 0.06779661 0.04166667]\n",
            "  [0.19444444 0.54166667 0.06779661 0.04166667]\n",
            "  [0.36111111 0.33333333 0.66101695 0.79166667]\n",
            "  [0.30555556 0.79166667 0.05084746 0.125     ]\n",
            "  [0.44444444 0.5        0.6440678  0.70833333]\n",
            "  [0.22222222 0.625      0.06779661 0.08333333]\n",
            "  [0.52777778 0.33333333 0.6440678  0.70833333]\n",
            "  [0.22222222 0.625      0.06779661 0.04166667]\n",
            "  [0.80555556 0.41666667 0.81355932 0.625     ]\n",
            "  [0.52777778 0.375      0.55932203 0.5       ]]\n",
            "\n",
            " [[0.19444444 0.625      0.05084746 0.08333333]\n",
            "  [0.02777778 0.5        0.05084746 0.04166667]\n",
            "  [0.63888889 0.41666667 0.57627119 0.54166667]\n",
            "  [0.16666667 0.66666667 0.06779661 0.        ]\n",
            "  [0.55555556 0.54166667 0.62711864 0.625     ]\n",
            "  [0.5        0.33333333 0.62711864 0.45833333]\n",
            "  [0.41666667 0.29166667 0.69491525 0.75      ]\n",
            "  [0.33333333 0.125      0.50847458 0.5       ]\n",
            "  [0.38888889 0.33333333 0.59322034 0.5       ]\n",
            "  [0.55555556 0.20833333 0.66101695 0.58333333]\n",
            "  [0.55555556 0.20833333 0.6779661  0.75      ]\n",
            "  [0.94444444 0.33333333 0.96610169 0.79166667]\n",
            "  [0.33333333 0.91666667 0.06779661 0.04166667]\n",
            "  [0.55555556 0.33333333 0.69491525 0.58333333]\n",
            "  [0.08333333 0.58333333 0.06779661 0.08333333]\n",
            "  [0.41666667 0.83333333 0.03389831 0.04166667]\n",
            "  [0.22222222 0.75       0.15254237 0.125     ]\n",
            "  [0.19444444 0.5        0.03389831 0.04166667]\n",
            "  [0.41666667 0.33333333 0.69491525 0.95833333]\n",
            "  [0.30555556 0.41666667 0.59322034 0.58333333]\n",
            "  [0.55555556 0.125      0.57627119 0.5       ]\n",
            "  [0.19444444 0.58333333 0.10169492 0.125     ]\n",
            "  [0.22222222 0.75       0.08474576 0.08333333]\n",
            "  [0.75       0.5        0.62711864 0.54166667]\n",
            "  [0.63888889 0.375      0.61016949 0.5       ]\n",
            "  [0.41666667 0.29166667 0.49152542 0.45833333]\n",
            "  [0.47222222 0.375      0.59322034 0.58333333]\n",
            "  [0.41666667 0.29166667 0.52542373 0.375     ]\n",
            "  [0.19444444 0.         0.42372881 0.375     ]\n",
            "  [0.33333333 0.625      0.05084746 0.04166667]]\n",
            "\n",
            " [[0.80555556 0.66666667 0.86440678 1.        ]\n",
            "  [0.16666667 0.20833333 0.59322034 0.66666667]\n",
            "  [0.94444444 0.41666667 0.86440678 0.91666667]\n",
            "  [0.41666667 0.29166667 0.69491525 0.75      ]\n",
            "  [0.22222222 0.54166667 0.11864407 0.16666667]\n",
            "  [0.72222222 0.5        0.79661017 0.91666667]\n",
            "  [0.25       0.625      0.08474576 0.04166667]\n",
            "  [0.86111111 0.33333333 0.86440678 0.75      ]\n",
            "  [0.58333333 0.5        0.59322034 0.58333333]\n",
            "  [0.58333333 0.375      0.55932203 0.5       ]\n",
            "  [0.         0.41666667 0.01694915 0.        ]\n",
            "  [0.36111111 0.375      0.44067797 0.5       ]\n",
            "  [0.22222222 0.75       0.10169492 0.04166667]\n",
            "  [0.5        0.375      0.62711864 0.54166667]\n",
            "  [1.         0.75       0.91525424 0.79166667]\n",
            "  [0.61111111 0.5        0.69491525 0.79166667]\n",
            "  [0.08333333 0.45833333 0.08474576 0.04166667]\n",
            "  [0.05555556 0.125      0.05084746 0.08333333]\n",
            "  [0.94444444 0.75       0.96610169 0.875     ]\n",
            "  [0.36111111 0.29166667 0.54237288 0.5       ]\n",
            "  [0.36111111 0.20833333 0.49152542 0.41666667]\n",
            "  [0.30555556 0.70833333 0.08474576 0.04166667]\n",
            "  [0.13888889 0.58333333 0.10169492 0.04166667]\n",
            "  [0.38888889 1.         0.08474576 0.125     ]\n",
            "  [0.66666667 0.45833333 0.57627119 0.54166667]\n",
            "  [0.66666667 0.41666667 0.71186441 0.91666667]\n",
            "  [0.16666667 0.45833333 0.08474576 0.        ]\n",
            "  [0.5        0.41666667 0.61016949 0.54166667]\n",
            "  [0.58333333 0.29166667 0.72881356 0.75      ]\n",
            "  [0.55555556 0.29166667 0.66101695 0.70833333]]] [[0 2 0 1 1 2 2 0 1 2 2 0 1 2 0 1 1 1 2 2 2 1 1 0 2 1 2 1 1 0 0 1 1 0 0 1\n",
            "  1 0 2 2 2 1 2 2 0 0 2 2 2 0 0 2 0 1 1 1 0 2 0 1 1 0 1 1 0 1 2 0 1 0 0 0\n",
            "  1 0 2 1 0 2 2 2 0 2 0 1 0 0 0 1 2 1 0 2 1 1 1 0 0 0 2 0 2 0 2 1 1 0 0 1\n",
            "  1 2 1 1 1 1 2 1 2 0 2 0]\n",
            " [0 2 0 2 1 2 2 1 2 2 0 1 2 0 0 1 1 1 2 2 2 2 1 0 2 0 1 2 1 1 0 0 1 1 0 0\n",
            "  0 1 1 1 0 2 2 2 2 1 2 2 0 0 2 2 0 2 0 2 0 0 1 0 2 0 1 2 0 1 1 1 1 0 1 2\n",
            "  1 2 1 0 1 0 0 0 0 1 2 0 2 2 2 0 1 0 0 2 1 1 1 0 0 0 2 2 2 2 2 0 2 1 1 2\n",
            "  0 1 1 2 0 1 1 0 1 1 2 0]\n",
            " [0 2 0 1 2 1 2 0 2 0 2 0 0 1 1 2 1 2 0 2 0 1 2 1 1 0 0 1 1 0 1 0 1 1 1 0\n",
            "  2 2 2 1 2 0 2 0 2 0 2 0 2 1 1 0 2 0 1 2 0 1 1 0 1 0 2 1 2 2 1 1 0 0 1 0\n",
            "  2 1 2 2 2 2 0 2 0 0 0 1 2 1 0 2 1 1 1 0 0 2 2 2 0 2 2 0 0 2 1 1 0 2 1 1\n",
            "  0 1 1 0 1 2 1 1 2 0 2 0]\n",
            " [2 1 2 2 2 0 1 2 2 1 2 0 0 1 2 2 2 1 2 1 0 0 2 1 0 0 0 1 1 0 1 0 0 1 1 0\n",
            "  2 2 1 2 2 0 2 0 2 2 0 2 0 1 1 1 0 2 0 1 2 1 1 1 0 1 1 1 2 1 2 2 1 0 1 0\n",
            "  0 0 1 0 2 1 2 0 2 2 2 2 0 2 0 1 0 0 1 0 2 1 1 0 2 2 2 0 2 2 2 0 0 2 0 2\n",
            "  0 2 0 1 1 0 1 2 1 2 0 2]\n",
            " [0 0 1 2 1 2 0 1 2 0 1 0 1 1 1 2 2 1 2 1 2 0 1 1 0 0 0 1 0 1 0 1 1 2 2 2\n",
            "  2 2 0 0 2 2 0 0 2 2 0 1 1 1 0 2 1 0 1 1 0 1 1 0 1 2 1 2 2 1 0 0 1 2 2 0\n",
            "  2 2 2 0 2 0 1 0 0 1 2 1 0 2 1 1 0 0 0 2 2 2 0 2 2 2 0 0 1 1 0 2 0 1 1 2\n",
            "  0 1 1 1 0 1 2 1 1 0 2 0]] [[2 0 2 0 0 1 0 1 2 0 2 0 2 1 1 2 1 2 1 2 2 2 2 2 2 0 2 0 0 1]\n",
            " [1 0 1 0 1 2 1 1 0 1 2 1 2 2 0 2 0 1 0 2 1 2 0 0 0 1 1 2 0 2]\n",
            " [2 1 2 1 1 2 2 1 0 0 2 2 0 2 0 1 1 1 1 0 0 0 2 0 1 0 2 0 2 1]\n",
            " [0 0 1 0 1 1 2 1 1 1 2 2 0 2 0 0 0 0 2 1 1 0 0 1 1 1 1 1 1 0]\n",
            " [2 2 2 2 0 2 0 2 1 1 0 1 0 1 2 2 0 0 2 1 1 0 0 0 1 2 0 1 2 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmTjzU336ijc"
      },
      "source": [
        "# One Hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Epwpp2SQ3bcJ"
      },
      "source": [
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "def one_hot_encoding(y):\n",
        "  label_encoder = LabelEncoder()\n",
        "  integer_encoded = label_encoder.fit_transform(y)\n",
        "  onehot_encoder = OneHotEncoder(sparse=False)\n",
        "  integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "  onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "  return onehot_encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDhKHXT9oX-q"
      },
      "source": [
        "# adding one column with all once\n",
        "def add_intercept(x):\n",
        "  intercept = np.ones((x.shape[0], 1))\n",
        "  return np.concatenate((intercept, x), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUME8sA95RXl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b239bc9-e9c6-41c7-b406-b27117b1edc1"
      },
      "source": [
        "onehot = one_hot_encoding(y_actual)\n",
        "print(onehot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CewazDMB6q8M"
      },
      "source": [
        "def loss(h, y):\n",
        "  return sum(-y*np.log(h) - (1-y)*np.log(1-h))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqUcyfAE6_BS"
      },
      "source": [
        "def mse(h, y):\n",
        "  return (((h-y)**2).mean())/2\n",
        "def mse_in_iter(h, y):\n",
        "  return (sum((h-y)**2))/2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n7atNv77RNW"
      },
      "source": [
        "def predict(x, y, all_weights):\n",
        "  # add intercept into x values\n",
        "  X = add_intercept(x)\n",
        "  y = []\n",
        "  for i in range(len(X)):\n",
        "    d1 = np.dot(all_weights[0], X[i].T)\n",
        "    d2 = np.dot(all_weights[1], X[i].T)\n",
        "    d3 = np.dot(all_weights[2], X[i].T)\n",
        "    if (d1 > d2 and d1 > d3):\n",
        "      y.append(0)\n",
        "    elif (d2 > d1 and d2 > d3):\n",
        "      y.append(1)\n",
        "    else :\n",
        "      y.append(2)\n",
        "  return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsOM0-A8pGmX"
      },
      "source": [
        "def slp(x, y, lr=0.01, roh = 0.000001, num_iter=100):\n",
        "  x = add_intercept(x) # add intercept in the feature vectors (in independent varibales)\n",
        "  all_hypothesis = []\n",
        "  all_weights = []\n",
        "  final_hypothesis = []\n",
        "  classes = list(set(y)) # find how many classes are there...\n",
        "  d = np.ones(len(classes))\n",
        "  # calculate the weight vector (2D array, shape will be (no_of_classes*no_of_feature_vector))\n",
        "  for i in range(0, len(classes)):\n",
        "    weight = np.ones(x.shape[1])*0.1\n",
        "    all_weights.append(weight)\n",
        "  all_weights = np.array(all_weights)\n",
        "  converged = True\n",
        "  epoch = 0\n",
        "  # make one hot encoding\n",
        "  y_onehot = one_hot_encoding(y)\n",
        "  current_loss = 0\n",
        "  while converged:\n",
        "    #logloss_error = 0\n",
        "    mse_error = 0   # at the starting of each epoch mse will be 0\n",
        "    for i in range(0, x.shape[0]): # for each training sample one by one\n",
        "      hypo = np.ones(len(classes))\n",
        "      for k in range(0, len(classes)): # for calculating d values\n",
        "        z = np.dot(x[i], all_weights[k].T)\n",
        "        #h = sigmoid(z)  # hypothesis\n",
        "        h = z\n",
        "        hypo[k] = h\n",
        "        if (h>=0):\n",
        "          d[k] = 1 # predicted one\n",
        "        if (h<0):\n",
        "          d[k] = 0 # predicted one\n",
        "      # calculate the errror (mse and logloss)\n",
        "      # but we are converging through the logloss only\n",
        "      mse_error = mse_error + mse_in_iter(d, y_onehot[i])\n",
        "      #logloss_error = logloss_error + loss(hypo, y_onehot[i])\n",
        "      # Weight updation using the current training sample\n",
        "      update_weights = []\n",
        "      for j in range(0, len(classes)):\n",
        "        all_weights[j] = all_weights[j] + x[i]*(y_onehot[i][j]-d[j])*lr\n",
        "    print(f'Epoch: {epoch} ------>' + f' ------> Mse: {mse_error/x.shape[0]} \\t ')\n",
        "    if(abs(current_loss - (mse_error/x.shape[0])) <= roh):\n",
        "        print(f\"Converged through roh criteria: epoch = {i}\")\n",
        "        break # converged\n",
        "    current_loss = mse_error/x.shape[0] # save the previous error to calculate the diff in current error and previous error\n",
        "    epoch = epoch + 1\n",
        "    if (epoch== num_iter):\n",
        "      print(\"Converged through maximum epoch no criteria...\") # converged\n",
        "      # return the traning accuracy also\n",
        "      return all_weights\n",
        "  # return the accuracy also\n",
        "  y_predicted = predict_inner(x, all_weights)\n",
        "  y_predicted = np.array(y_predicted)\n",
        "  acc = accuracy_score(y, y_predicted)*100\n",
        "  print(\"Train Accuracy: \" + str(acc))\n",
        "  return all_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGpU555bLp59",
        "outputId": "b59758be-e6ad-4601-db66-91c0002a97df"
      },
      "source": [
        "all_weights = slp(X_train, y_train, lr=0.3, roh = 0.000001, num_iter=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 ------> ------> Mse: 0.3952380952380952 \t \n",
            "Epoch: 1 ------> ------> Mse: 0.2714285714285714 \t \n",
            "Epoch: 2 ------> ------> Mse: 0.2904761904761905 \t \n",
            "Epoch: 3 ------> ------> Mse: 0.26666666666666666 \t \n",
            "Epoch: 4 ------> ------> Mse: 0.23333333333333334 \t \n",
            "Epoch: 5 ------> ------> Mse: 0.2571428571428571 \t \n",
            "Epoch: 6 ------> ------> Mse: 0.21904761904761905 \t \n",
            "Epoch: 7 ------> ------> Mse: 0.20952380952380953 \t \n",
            "Epoch: 8 ------> ------> Mse: 0.2523809523809524 \t \n",
            "Epoch: 9 ------> ------> Mse: 0.21904761904761905 \t \n",
            "Epoch: 10 ------> ------> Mse: 0.2523809523809524 \t \n",
            "Epoch: 11 ------> ------> Mse: 0.21904761904761905 \t \n",
            "Epoch: 12 ------> ------> Mse: 0.23809523809523808 \t \n",
            "Epoch: 13 ------> ------> Mse: 0.20952380952380953 \t \n",
            "Epoch: 14 ------> ------> Mse: 0.18095238095238095 \t \n",
            "Epoch: 15 ------> ------> Mse: 0.20952380952380953 \t \n",
            "Epoch: 16 ------> ------> Mse: 0.19047619047619047 \t \n",
            "Epoch: 17 ------> ------> Mse: 0.20952380952380953 \t \n",
            "Epoch: 18 ------> ------> Mse: 0.18571428571428572 \t \n",
            "Epoch: 19 ------> ------> Mse: 0.20952380952380953 \t \n",
            "Epoch: 20 ------> ------> Mse: 0.19047619047619047 \t \n",
            "Epoch: 21 ------> ------> Mse: 0.20952380952380953 \t \n",
            "Epoch: 22 ------> ------> Mse: 0.20952380952380953 \t \n",
            "Converged through roh criteria: epoch = 104\n",
            "Train Accuracy: 74.28571428571429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZPRXL1fbj7t",
        "outputId": "a7028bcc-93ad-4134-ba15-a3e589a20e84"
      },
      "source": [
        "y_predicted = predict(X_train, y_train, all_weights)\n",
        "y_predicted = np.array(y_predicted)\n",
        "print(accuracy_score(y_train , y_predicted))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7428571428571429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtR4w-7TNK-2",
        "outputId": "32808867-baec-469d-876d-c7c4f4702c1d"
      },
      "source": [
        "y_predicted = predict(X_test, y_test, all_weights)\n",
        "y_predicted = np.array(y_predicted)\n",
        "print(accuracy_score(y_test , y_predicted ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8421052631578947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyBehvAWNLBw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ab7ac93-98ea-4ffe-cdf6-94cb960a1532"
      },
      "source": [
        "y_predicted = predict(X_validation, y_validation, all_weights)\n",
        "y_predicted = np.array(y_predicted)\n",
        "print(accuracy_score(y_validation, y_predicted ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGU1JgP4urNM",
        "outputId": "47a06eea-9c88-429c-cdf2-983294e4556a"
      },
      "source": [
        "# Take average of all folds\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn import metrics\n",
        "accuracy = np.ones(5)\n",
        "precision = np.ones(5)\n",
        "recall = np.ones(5)\n",
        "f1 = np.ones(5)\n",
        "for i in range(0,5):\n",
        "  all_weights = slp(all_x_train[i], all_y_train[i], lr=0.3, roh = 0.000001, num_iter=100)\n",
        "  y_predicted = predict(all_x_test[i], all_y_test[i], all_weights)\n",
        "  y_predicted = np.array(y_predicted)\n",
        "  accuracy[i] = accuracy_score(all_y_test[i] , y_predicted)*100\n",
        "  print(\"Accuracy Score:\", accuracy[i])\n",
        "  precision[i] = precision_score(all_y_test[i], y_predicted, labels=[0,1,2], average='micro')*100\n",
        "  recall[i] = recall_score(all_y_test[i], y_predicted, labels=[0,1,2], average='micro')*100\n",
        "  f1[i] = f1_score(all_y_test[i], y_predicted, labels=[0,1,2], average='micro')*100\n",
        "  print(\"Training Accuracy: \", accuracy[i])\n",
        "  print(\"Prescission Score:\", precision[i])\n",
        "  print(\"Recall Score : \", recall_score(all_y_test[i], y_predicted, labels=[0,1,2], average='micro')*100) \n",
        "  print('F1 Score : ', f1_score(all_y_test[i], y_predicted, labels=[0,1,2], average='micro')*100)\n",
        "  print('Confusion Matrix : \\n' + str(confusion_matrix(all_y_test[i], y_predicted)))\n",
        "  print(\"Classification Report for 3-classes: \")\n",
        "  out_labels = [0,1,2]\n",
        "  print(metrics.classification_report(all_y_test[i], y_predicted, out_labels, digits=3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 ------> ------> Mse: 0.30416666666666664 \t \n",
            "Epoch: 1 ------> ------> Mse: 0.2625 \t \n",
            "Epoch: 2 ------> ------> Mse: 0.22916666666666666 \t \n",
            "Epoch: 3 ------> ------> Mse: 0.24166666666666667 \t \n",
            "Epoch: 4 ------> ------> Mse: 0.2708333333333333 \t \n",
            "Epoch: 5 ------> ------> Mse: 0.22916666666666666 \t \n",
            "Epoch: 6 ------> ------> Mse: 0.2 \t \n",
            "Epoch: 7 ------> ------> Mse: 0.19166666666666668 \t \n",
            "Epoch: 8 ------> ------> Mse: 0.17916666666666667 \t \n",
            "Epoch: 9 ------> ------> Mse: 0.21666666666666667 \t \n",
            "Epoch: 10 ------> ------> Mse: 0.21666666666666667 \t \n",
            "Converged through roh criteria: epoch = 119\n",
            "Train Accuracy: 75.83333333333333\n",
            "Accuracy Score: 93.33333333333333\n",
            "Training Accuracy:  93.33333333333333\n",
            "Prescission Score: 93.33333333333333\n",
            "Recall Score :  93.33333333333333\n",
            "F1 Score :  93.33333333333333\n",
            "Confusion Matrix : \n",
            "[[ 9  0  0]\n",
            " [ 2  5  0]\n",
            " [ 0  0 14]]\n",
            "Classification Report for 3-classes: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.818     1.000     0.900         9\n",
            "           1      1.000     0.714     0.833         7\n",
            "           2      1.000     1.000     1.000        14\n",
            "\n",
            "    accuracy                          0.933        30\n",
            "   macro avg      0.939     0.905     0.911        30\n",
            "weighted avg      0.945     0.933     0.931        30\n",
            "\n",
            "Epoch: 0 ------> ------> Mse: 0.30416666666666664 \t \n",
            "Epoch: 1 ------> ------> Mse: 0.25416666666666665 \t \n",
            "Epoch: 2 ------> ------> Mse: 0.2375 \t \n",
            "Epoch: 3 ------> ------> Mse: 0.2125 \t \n",
            "Epoch: 4 ------> ------> Mse: 0.20833333333333334 \t \n",
            "Epoch: 5 ------> ------> Mse: 0.23333333333333334 \t \n",
            "Epoch: 6 ------> ------> Mse: 0.2125 \t \n",
            "Epoch: 7 ------> ------> Mse: 0.20833333333333334 \t \n",
            "Epoch: 8 ------> ------> Mse: 0.225 \t \n",
            "Epoch: 9 ------> ------> Mse: 0.22916666666666666 \t \n",
            "Epoch: 10 ------> ------> Mse: 0.20833333333333334 \t \n",
            "Epoch: 11 ------> ------> Mse: 0.19166666666666668 \t \n",
            "Epoch: 12 ------> ------> Mse: 0.20833333333333334 \t \n",
            "Epoch: 13 ------> ------> Mse: 0.2 \t \n",
            "Epoch: 14 ------> ------> Mse: 0.20833333333333334 \t \n",
            "Epoch: 15 ------> ------> Mse: 0.2 \t \n",
            "Epoch: 16 ------> ------> Mse: 0.19166666666666668 \t \n",
            "Epoch: 17 ------> ------> Mse: 0.20416666666666666 \t \n",
            "Epoch: 18 ------> ------> Mse: 0.19166666666666668 \t \n",
            "Epoch: 19 ------> ------> Mse: 0.20833333333333334 \t \n",
            "Epoch: 20 ------> ------> Mse: 0.20833333333333334 \t \n",
            "Converged through roh criteria: epoch = 119\n",
            "Train Accuracy: 80.83333333333333\n",
            "Accuracy Score: 70.0\n",
            "Training Accuracy:  70.0\n",
            "Prescission Score: 70.0\n",
            "Recall Score :  70.0\n",
            "F1 Score :  70.0\n",
            "Confusion Matrix : \n",
            "[[10  0  0]\n",
            " [ 5  2  4]\n",
            " [ 0  0  9]]\n",
            "Classification Report for 3-classes: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.667     1.000     0.800        10\n",
            "           1      1.000     0.182     0.308        11\n",
            "           2      0.692     1.000     0.818         9\n",
            "\n",
            "    accuracy                          0.700        30\n",
            "   macro avg      0.786     0.727     0.642        30\n",
            "weighted avg      0.797     0.700     0.625        30\n",
            "\n",
            "Epoch: 0 ------> ------> Mse: 0.30833333333333335 \t \n",
            "Epoch: 1 ------> ------> Mse: 0.25 \t \n",
            "Epoch: 2 ------> ------> Mse: 0.24166666666666667 \t \n",
            "Epoch: 3 ------> ------> Mse: 0.18333333333333332 \t \n",
            "Epoch: 4 ------> ------> Mse: 0.19166666666666668 \t \n",
            "Epoch: 5 ------> ------> Mse: 0.2 \t \n",
            "Epoch: 6 ------> ------> Mse: 0.17916666666666667 \t \n",
            "Epoch: 7 ------> ------> Mse: 0.1625 \t \n",
            "Epoch: 8 ------> ------> Mse: 0.15833333333333333 \t \n",
            "Epoch: 9 ------> ------> Mse: 0.15416666666666667 \t \n",
            "Epoch: 10 ------> ------> Mse: 0.1625 \t \n",
            "Epoch: 11 ------> ------> Mse: 0.175 \t \n",
            "Epoch: 12 ------> ------> Mse: 0.15 \t \n",
            "Epoch: 13 ------> ------> Mse: 0.1625 \t \n",
            "Epoch: 14 ------> ------> Mse: 0.14166666666666666 \t \n",
            "Epoch: 15 ------> ------> Mse: 0.1625 \t \n",
            "Epoch: 16 ------> ------> Mse: 0.15833333333333333 \t \n",
            "Epoch: 17 ------> ------> Mse: 0.16666666666666666 \t \n",
            "Epoch: 18 ------> ------> Mse: 0.2 \t \n",
            "Epoch: 19 ------> ------> Mse: 0.17916666666666667 \t \n",
            "Epoch: 20 ------> ------> Mse: 0.18333333333333332 \t \n",
            "Epoch: 21 ------> ------> Mse: 0.2 \t \n",
            "Epoch: 22 ------> ------> Mse: 0.15 \t \n",
            "Epoch: 23 ------> ------> Mse: 0.15833333333333333 \t \n",
            "Epoch: 24 ------> ------> Mse: 0.16666666666666666 \t \n",
            "Epoch: 25 ------> ------> Mse: 0.16666666666666666 \t \n",
            "Converged through roh criteria: epoch = 119\n",
            "Train Accuracy: 68.33333333333333\n",
            "Accuracy Score: 66.66666666666666\n",
            "Training Accuracy:  66.66666666666666\n",
            "Prescission Score: 66.66666666666666\n",
            "Recall Score :  66.66666666666666\n",
            "F1 Score :  66.66666666666666\n",
            "Confusion Matrix : \n",
            "[[10  0  0]\n",
            " [ 7  0  3]\n",
            " [ 0  0 10]]\n",
            "Classification Report for 3-classes: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.588     1.000     0.741        10\n",
            "           1      0.000     0.000     0.000        10\n",
            "           2      0.769     1.000     0.870        10\n",
            "\n",
            "    accuracy                          0.667        30\n",
            "   macro avg      0.452     0.667     0.537        30\n",
            "weighted avg      0.452     0.667     0.537        30\n",
            "\n",
            "Epoch: 0 ------> ------> Mse: 0.29583333333333334 \t \n",
            "Epoch: 1 ------> ------> Mse: 0.2625 \t \n",
            "Epoch: 2 ------> ------> Mse: 0.22916666666666666 \t \n",
            "Epoch: 3 ------> ------> Mse: 0.21666666666666667 \t \n",
            "Epoch: 4 ------> ------> Mse: 0.22916666666666666 \t \n",
            "Epoch: 5 ------> ------> Mse: 0.19166666666666668 \t \n",
            "Epoch: 6 ------> ------> Mse: 0.18333333333333332 \t \n",
            "Epoch: 7 ------> ------> Mse: 0.19583333333333333 \t \n",
            "Epoch: 8 ------> ------> Mse: 0.2 \t \n",
            "Epoch: 9 ------> ------> Mse: 0.19583333333333333 \t \n",
            "Epoch: 10 ------> ------> Mse: 0.20833333333333334 \t \n",
            "Epoch: 11 ------> ------> Mse: 0.2 \t \n",
            "Epoch: 12 ------> ------> Mse: 0.18333333333333332 \t \n",
            "Epoch: 13 ------> ------> Mse: 0.20416666666666666 \t \n",
            "Epoch: 14 ------> ------> Mse: 0.20833333333333334 \t \n",
            "Epoch: 15 ------> ------> Mse: 0.19166666666666668 \t \n",
            "Epoch: 16 ------> ------> Mse: 0.19166666666666668 \t \n",
            "Converged through roh criteria: epoch = 119\n",
            "Train Accuracy: 70.0\n",
            "Accuracy Score: 50.0\n",
            "Training Accuracy:  50.0\n",
            "Prescission Score: 50.0\n",
            "Recall Score :  50.0\n",
            "F1 Score :  50.0\n",
            "Confusion Matrix : \n",
            "[[11  0  0]\n",
            " [13  0  1]\n",
            " [ 1  0  4]]\n",
            "Classification Report for 3-classes: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.440     1.000     0.611        11\n",
            "           1      0.000     0.000     0.000        14\n",
            "           2      0.800     0.800     0.800         5\n",
            "\n",
            "    accuracy                          0.500        30\n",
            "   macro avg      0.413     0.600     0.470        30\n",
            "weighted avg      0.295     0.500     0.357        30\n",
            "\n",
            "Epoch: 0 ------> ------> Mse: 0.30833333333333335 \t \n",
            "Epoch: 1 ------> ------> Mse: 0.225 \t \n",
            "Epoch: 2 ------> ------> Mse: 0.25 \t \n",
            "Epoch: 3 ------> ------> Mse: 0.225 \t \n",
            "Epoch: 4 ------> ------> Mse: 0.20416666666666666 \t \n",
            "Epoch: 5 ------> ------> Mse: 0.1625 \t \n",
            "Epoch: 6 ------> ------> Mse: 0.15 \t \n",
            "Epoch: 7 ------> ------> Mse: 0.15833333333333333 \t \n",
            "Epoch: 8 ------> ------> Mse: 0.16666666666666666 \t \n",
            "Epoch: 9 ------> ------> Mse: 0.1875 \t \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10 ------> ------> Mse: 0.15833333333333333 \t \n",
            "Epoch: 11 ------> ------> Mse: 0.15833333333333333 \t \n",
            "Converged through roh criteria: epoch = 119\n",
            "Train Accuracy: 77.5\n",
            "Accuracy Score: 73.33333333333333\n",
            "Training Accuracy:  73.33333333333333\n",
            "Prescission Score: 73.33333333333333\n",
            "Recall Score :  73.33333333333333\n",
            "F1 Score :  73.33333333333333\n",
            "Confusion Matrix : \n",
            "[[ 9  1  0]\n",
            " [ 6  2  0]\n",
            " [ 0  1 11]]\n",
            "Classification Report for 3-classes: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.600     0.900     0.720        10\n",
            "           1      0.500     0.250     0.333         8\n",
            "           2      1.000     0.917     0.957        12\n",
            "\n",
            "    accuracy                          0.733        30\n",
            "   macro avg      0.700     0.689     0.670        30\n",
            "weighted avg      0.733     0.733     0.711        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyPJIIkfwmS4",
        "outputId": "e66d19d6-1b2d-4cdc-e428-51c4a39f7304"
      },
      "source": [
        "print(\"Mean Accuracy: \",accuracy.mean())\n",
        "print(\"Mean precission: \", precision.mean())\n",
        "print(\"Mean recall: \",recall.mean())\n",
        "print(\"Mean f1 score: \",f1.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Accuracy:  70.66666666666666\n",
            "Mean precission:  70.66666666666666\n",
            "Mean recall:  70.66666666666666\n",
            "Mean f1 score:  70.66666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPgP1TYQddxZ"
      },
      "source": [
        "# Hyperparameter tuning on the validation set is required to be done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQTFoM-WwrfX",
        "outputId": "4fafaf2e-f259-4053-dad2-e47c25433817"
      },
      "source": [
        "# Do the splitting on the first fold\n",
        "# Train test validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "# train(70%), validation (10%), and test(20%)\n",
        "X_train, X_validation, y_train, y_validation = train_test_split(all_x_train[0], all_y_train[0] , test_size=0.1, random_state=random.randint(30,100))\n",
        "print(\"Size of the train dataset:\" + str(X_train.shape))\n",
        "print(\"Size of the validation dataset:\" + str(X_validation.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the train dataset:(108, 4)\n",
            "Size of the validation dataset:(12, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxAcqlRjw6uL"
      },
      "source": [
        "# Hyperparameter tuning on the validation set is required to be done.\n",
        "# here we are choosing the best hyperparameters based on the accuracy value\n",
        "def get_best_hyperparameter(X_train, y_train, X_validation, y_validation, alpha, roh, num_iter):\n",
        "  valid_hypothesis = []\n",
        "  validation_accuracy = []\n",
        "  x = add_intercept(X_validation)\n",
        "  for i in range(0, len(alpha)):\n",
        "    all_hypothesis_valid = []\n",
        "    final_hypothesis = []\n",
        "    all_weights = slp(X_train, y_train, lr=alpha[i], roh = roh[i], num_iter=num_iter[i])\n",
        "    y_predicted = predict(X_validation, y_validation, all_weights)\n",
        "    y_predicted = np.array(y_predicted)\n",
        "    accuracy[i] = accuracy_score(y_validation, y_predicted)*100\n",
        "    print(\"Validation Accuracy: \" + str(accuracy[i]))\n",
        "    # calculate the accuracy in the validation set\n",
        "  max_index = np.argmax(accuracy)\n",
        "  print(\"Best hyperparameter value is alpha = \" + str(alpha[max_index]) + \", roh = \" + str(roh[max_index]) + \" and num_iter = \" + str(num_iter[max_index]))\n",
        "  print(\"Best Hyperparameter accuracy: \" + str(accuracy[max_index]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUu62umKxIu3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dd8339c-14ac-4051-d801-d913e3878ab1"
      },
      "source": [
        "alpha = [0.01, 0.0001, 0.1, 0.2]\n",
        "roh =   [0.0001, 0.0000001, 0.000000001, 0.0000000000001]\n",
        "num_iter = [10, 20, 30, 40]\n",
        "get_best_hyperparameter(X_train, y_train, X_validation, y_validation,alpha, roh, num_iter)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 ------> ------> Mse: 0.4675925925925926 \t \n",
            "Epoch: 1 ------> ------> Mse: 0.2222222222222222 \t \n",
            "Epoch: 2 ------> ------> Mse: 0.23148148148148148 \t \n",
            "Epoch: 3 ------> ------> Mse: 0.19907407407407407 \t \n",
            "Epoch: 4 ------> ------> Mse: 0.18981481481481483 \t \n",
            "Epoch: 5 ------> ------> Mse: 0.18981481481481483 \t \n",
            "Converged through roh criteria: epoch = 107\n",
            "Train Accuracy: 84.25925925925925\n",
            "Validation Accuracy: 83.33333333333334\n",
            "Epoch: 0 ------> ------> Mse: 1.0 \t \n",
            "Epoch: 1 ------> ------> Mse: 1.0 \t \n",
            "Converged through roh criteria: epoch = 107\n",
            "Train Accuracy: 68.51851851851852\n",
            "Validation Accuracy: 50.0\n",
            "Epoch: 0 ------> ------> Mse: 0.25 \t \n",
            "Epoch: 1 ------> ------> Mse: 0.22685185185185186 \t \n",
            "Epoch: 2 ------> ------> Mse: 0.20833333333333334 \t \n",
            "Epoch: 3 ------> ------> Mse: 0.19907407407407407 \t \n",
            "Epoch: 4 ------> ------> Mse: 0.2361111111111111 \t \n",
            "Epoch: 5 ------> ------> Mse: 0.2175925925925926 \t \n",
            "Epoch: 6 ------> ------> Mse: 0.20833333333333334 \t \n",
            "Epoch: 7 ------> ------> Mse: 0.17592592592592593 \t \n",
            "Epoch: 8 ------> ------> Mse: 0.20833333333333334 \t \n",
            "Epoch: 9 ------> ------> Mse: 0.2361111111111111 \t \n",
            "Epoch: 10 ------> ------> Mse: 0.18981481481481483 \t \n",
            "Epoch: 11 ------> ------> Mse: 0.19907407407407407 \t \n",
            "Epoch: 12 ------> ------> Mse: 0.2222222222222222 \t \n",
            "Epoch: 13 ------> ------> Mse: 0.2037037037037037 \t \n",
            "Epoch: 14 ------> ------> Mse: 0.2037037037037037 \t \n",
            "Converged through roh criteria: epoch = 107\n",
            "Train Accuracy: 86.11111111111111\n",
            "Validation Accuracy: 83.33333333333334\n",
            "Epoch: 0 ------> ------> Mse: 0.2824074074074074 \t \n",
            "Epoch: 1 ------> ------> Mse: 0.2638888888888889 \t \n",
            "Epoch: 2 ------> ------> Mse: 0.18518518518518517 \t \n",
            "Epoch: 3 ------> ------> Mse: 0.20833333333333334 \t \n",
            "Epoch: 4 ------> ------> Mse: 0.2222222222222222 \t \n",
            "Epoch: 5 ------> ------> Mse: 0.19444444444444445 \t \n",
            "Epoch: 6 ------> ------> Mse: 0.20833333333333334 \t \n",
            "Epoch: 7 ------> ------> Mse: 0.2037037037037037 \t \n",
            "Epoch: 8 ------> ------> Mse: 0.17592592592592593 \t \n",
            "Epoch: 9 ------> ------> Mse: 0.2037037037037037 \t \n",
            "Epoch: 10 ------> ------> Mse: 0.20833333333333334 \t \n",
            "Epoch: 11 ------> ------> Mse: 0.17592592592592593 \t \n",
            "Epoch: 12 ------> ------> Mse: 0.14814814814814814 \t \n",
            "Epoch: 13 ------> ------> Mse: 0.19444444444444445 \t \n",
            "Epoch: 14 ------> ------> Mse: 0.1527777777777778 \t \n",
            "Epoch: 15 ------> ------> Mse: 0.19444444444444445 \t \n",
            "Epoch: 16 ------> ------> Mse: 0.19444444444444445 \t \n",
            "Converged through roh criteria: epoch = 107\n",
            "Train Accuracy: 87.96296296296296\n",
            "Validation Accuracy: 83.33333333333334\n",
            "Best hyperparameter value is alpha = 0.01, roh = 0.0001 and num_iter = 10\n",
            "Best Hyperparameter accuracy: 83.33333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxCj_vL_2iXJ"
      },
      "source": [
        "# Overfitting Detection\n",
        "Check for overfitting on the validation set by plotting graph during training  of the finalized model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKP_mYB12eJY"
      },
      "source": [
        "def plotting(x, y_1, y_2, label_1, label_2, t):\n",
        "      plt.plot(x, y_1, label = label_1)\n",
        "      plt.plot(x, y_2, label = label_2)\n",
        "      plt.title(t)\n",
        "      plt.legend()\n",
        "      plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qi3_LcnM-GZD"
      },
      "source": [
        "def predict_inner(X, all_weights):\n",
        "  # add intercept into x values\n",
        "  #X = add_intercept(x)\n",
        "  y = []\n",
        "  for i in range(len(X)):\n",
        "    d1 = np.dot(all_weights[0], X[i].T)\n",
        "    d2 = np.dot(all_weights[1], X[i].T)\n",
        "    d3 = np.dot(all_weights[2], X[i].T)\n",
        "    if (d1 > d2 and d1 > d3):\n",
        "      y.append(0)\n",
        "    elif (d2 > d1 and d2 > d3):\n",
        "      y.append(1)\n",
        "    else :\n",
        "      y.append(2)\n",
        "  return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDH9R9hI28qB"
      },
      "source": [
        "def slp_overfitting_detection(x, y, x_valid, y_valid, lr=0.01, roh = 0.000001, num_iter=100):\n",
        "  x = add_intercept(x) # add intercept in the feature vectors (in independent varibales)\n",
        "  x_valid = add_intercept(x_valid)\n",
        "  all_hypothesis = []\n",
        "  all_weights = []\n",
        "  final_hypothesis = []\n",
        "  train_logloss = []\n",
        "  train_mse = []\n",
        "  validation_logloss = []\n",
        "  validation_mse = []\n",
        "  classes = list(set(y)) # find how many classes are there...\n",
        "  d = np.ones(len(classes))\n",
        "  d_valid = np.ones(len(classes))\n",
        "  # calculate the weight vector (2D array, shape will be (no_of_classes*no_of_feature_vector))\n",
        "  for i in range(0, len(classes)):\n",
        "    weight = np.ones(x.shape[1])*0.1\n",
        "    all_weights.append(weight)\n",
        "  all_weights = np.array(all_weights)\n",
        "  converged = True\n",
        "  epoch = 0\n",
        "  # make one hot encoding\n",
        "  y_onehot = one_hot_encoding(y)\n",
        "  y_valid_onehot = one_hot_encoding(y_valid)\n",
        "  current_loss = 0\n",
        "  while converged:\n",
        "    logloss_error = 0\n",
        "    mse_error = 0   # at the starting of each epoch mse will be 0\n",
        "    valid_mse_error = 0\n",
        "    #----------------------------     Training Sample   ------------------------\n",
        "    for i in range(0, x.shape[0]): # for each training sample one by one\n",
        "      hypo = np.ones(len(classes))\n",
        "      for k in range(0, len(classes)): # for calculating d values\n",
        "        z = np.dot(x[i], all_weights[k].T)\n",
        "        #h = sigmoid(z)  # hypothesis\n",
        "        h = z\n",
        "        hypo[k] = h\n",
        "        if (h>=0):\n",
        "          d[k] = 1 # predicted one\n",
        "        if (h<0):\n",
        "          d[k] = 0 # predicted one\n",
        "      # calculate the errror (mse and logloss)\n",
        "      # but we are converging through the logloss only\n",
        "      mse_error = mse_error + mse_in_iter(d, y_onehot[i])\n",
        "      #logloss_error = logloss_error + loss(hypo, y_onehot[i])\n",
        "      # Weight updation using the current training sample\n",
        "      update_weights = []\n",
        "      for j in range(0, len(classes)):\n",
        "        all_weights[j] = all_weights[j] + x[i]*(y_onehot[i][j]-d[j])*lr\n",
        "    #---------------------------------------------------------------------------\n",
        "    # store train and validation mse epoch by epoch\n",
        "    #train_logloss.append(logloss_error)\n",
        "    train_mse.append(mse_error)\n",
        "    #-----------------       check for the validation   ------------------------\n",
        "    for i in range(0, x_valid.shape[0]): # for each training sample one by one\n",
        "      hypo_valid = np.ones(len(classes))\n",
        "      for k in range(0, len(classes)): # for calculating d values\n",
        "        z_valid = np.dot(x_valid[i], all_weights[k].T)\n",
        "        #h_valid = sigmoid(z_valid)  # hypothesis\n",
        "        h_valid = z_valid\n",
        "        hypo_valid[k] = h_valid\n",
        "        if (h_valid>=0):\n",
        "          d_valid[k] = 1 # predicted one\n",
        "        if (h_valid<0):\n",
        "          d_valid[k] = 0 # predicted one\n",
        "      valid_mse_error = valid_mse_error + mse_in_iter(d_valid, y_valid_onehot[i])\n",
        "      #valid_logloss_error = logloss_error + loss(hypo_valid, y_valid_onehot[i])\n",
        "    #validation_logloss.append(valid_logloss_error)\n",
        "    validation_mse.append(valid_mse_error)\n",
        "    print(f'Epoch: {epoch} ------>' + f' ------> Mse: {mse_error/x.shape[0]} \\t ')\n",
        "    if(abs(current_loss - (mse_error/x.shape[0])) <= roh):\n",
        "        print(f\"Converged through roh criteria: epoch = {i}\")\n",
        "        break # converged\n",
        "    current_loss = mse_error/x.shape[0] # save the previous error to calculate the diff in current error and previous error\n",
        "    epoch = epoch + 1\n",
        "    if (epoch== num_iter):\n",
        "      print(\"Converged through maximum epoch no criteria...\") # converged\n",
        "      # return the traning accuracy also\n",
        "      return all_weights\n",
        "  #train_logloss = np.array(train_logloss)\n",
        "  train_mse = np.array(train_mse)\n",
        "  #validation_logloss =  np.array(validation_logloss)\n",
        "  validation_mse = np.array(validation_mse)\n",
        "  plot_x = []\n",
        "  for i in range(0, epoch+1):\n",
        "    plot_x.append(i+1)\n",
        "  # plot here\n",
        "  plotting(plot_x, train_mse, validation_mse, \"Train Mse\",\"Validation Mse\", \"Epoch vs Training and Validation Mse\")\n",
        "  print(validation_mse)\n",
        "  print(train_mse)\n",
        "  #plotting(plot_x, train_logloss, validation_logloss, \"Train Logloss\",\"Validation Logloss\", \"Epoch vs Training and Validation Logloss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1H-5d5J0kDc",
        "outputId": "28a329a8-a50e-4fb6-934e-dd21e58110d3"
      },
      "source": [
        "# Do the splitting on the first fold\n",
        "# Train test validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "# train(70%), validation (10%), and test(20%)\n",
        "X_train, X_validation, y_train, y_validation = train_test_split(all_x_train[0], all_y_train[0] , test_size=0.2, random_state=random.randint(30,100))\n",
        "print(\"Size of the train dataset:\" + str(X_train.shape))\n",
        "print(\"Size of the validation dataset:\" + str(X_validation.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the train dataset:(96, 4)\n",
            "Size of the validation dataset:(24, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "ndyDzBSm3Ag_",
        "outputId": "ce1753bb-b9f0-461b-f0fa-dfd582501a48"
      },
      "source": [
        "slp_overfitting_detection(X_train, y_train, X_validation, y_validation, lr=0.01, roh = 0.000001, num_iter=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 ------> ------> Mse: 0.484375 \t \n",
            "Epoch: 1 ------> ------> Mse: 0.2708333333333333 \t \n",
            "Epoch: 2 ------> ------> Mse: 0.25 \t \n",
            "Epoch: 3 ------> ------> Mse: 0.265625 \t \n",
            "Epoch: 4 ------> ------> Mse: 0.25 \t \n",
            "Epoch: 5 ------> ------> Mse: 0.21875 \t \n",
            "Epoch: 6 ------> ------> Mse: 0.21875 \t \n",
            "Converged through roh criteria: epoch = 23\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6.5 6.5 6.5 6.5 6.5 7.  6.5]\n",
            "[46.5 26.  24.  25.5 24.  21.  21. ]\n"
          ]
        }
      ]
    }
  ]
}